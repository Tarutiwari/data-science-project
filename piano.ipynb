{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1yZY92VejMSZpkm7-dFft7IhxTVWDEQ68","authorship_tag":"ABX9TyN7UhGsXW4ISEfnzTUFogJx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/Tarutiwari/data-science-project.git"],"metadata":{"id":"9EFk9MipN2ev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746899554810,"user_tz":-330,"elapsed":13,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"6d3a33b9-1083-4b34-dfad-542d21309d54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'data-science-project' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiRopXYf9vl6"},"outputs":[],"source":["import tensorflow\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":[],"metadata":{"id":"Yz-vvGzXN1a5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","import random\n","import IPython\n","from IPython.display import Image, Audio\n","import music21\n","from music21 import *\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout"],"metadata":{"id":"SRbgbrqv_Be_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow.keras.backend as K\n","from tensorflow.keras.optimizers import Adamax\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","%matplotlib inline"],"metadata":{"id":"hdn789-6_Gk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.simplefilter(\"ignore\")\n","np.random.seed(42)"],"metadata":{"id":"yfQdrZWC_Kw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from music21 import converter"],"metadata":{"id":"CdrlgcdR_Q4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"BNln80yeCNfI"}},{"cell_type":"code","source":["folder_path = '/content/drive/MyDrive/music/solobeeeee'"],"metadata":{"id":"RWBb0XNsU1K5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install mido\n","!pip install pretty_midi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHgatHaAYOkB","executionInfo":{"status":"ok","timestamp":1745829219632,"user_tz":-330,"elapsed":8424,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"4f08faee-efbc-4684-8c52-ed8ad774781f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mido\n","  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido) (24.2)\n","Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: mido\n","Successfully installed mido-1.3.3\n","Collecting pretty_midi\n","  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n","Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\n","Building wheels for collected packages: pretty_midi\n","  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=2f63beba5f29eefeb107d29ed200d1b4197bff1f5b3deaa66dd307ea142ad167\n","  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n","Successfully built pretty_midi\n","Installing collected packages: pretty_midi\n","Successfully installed pretty_midi-0.2.10\n"]}]},{"cell_type":"code","source":["\n","#Getting midi files\n","all_midis = []\n","for i in os.listdir(folder_path):  # Use the folder path here\n","    if i.endswith(\".mid\"):  # Check for MIDI files\n","        file_path = os.path.join(folder_path, i)  # Correctly join folder path and file name\n","        midi = converter.parse(file_path)  # Parse MIDI file\n","        all_midis.append(midi)\n"],"metadata":{"id":"TVx9MX5YYRt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from music21 import converter, instrument, note, chord"],"metadata":{"id":"nA_32iAfPEn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_notes(file):\n","    notes = []\n","    pick = None\n","    for j in file:\n","        songs = instrument.partitionByInstrument(j)\n","        for part in songs.parts:\n","            pick = part.recurse()\n","\n","            for element in pick:\n","                if isinstance(element, note.Note):\n","                    notes.append(str(element.pitch))\n","                elif isinstance(element, chord.Chord):\n","                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n","\n","    return notes\n","\n","# Getting the list of notes as Corpus\n","Corpus = extract_notes(all_midis)\n","print(\"Total notes in all the MIDI files in the dataset:\", len(Corpus))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKKuk_hdPFi8","executionInfo":{"status":"ok","timestamp":1745829814961,"user_tz":-330,"elapsed":51708,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"07618968-f68b-4c7d-9df8-8b09fb95affc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total notes in all the MIDI files in the dataset: 156895\n"]}]},{"cell_type":"code","source":["print(\"First fifty values in the Corpus:\", Corpus[:50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4Q8Ln7uP3wI","executionInfo":{"status":"ok","timestamp":1745829814967,"user_tz":-330,"elapsed":31,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"b0347e13-4e67-47c3-9219-fbe97273d5a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First fifty values in the Corpus: ['G5', 'D5', 'B4', 'E-5', 'C5', 'G5', 'A4', 'F#5', 'D5', 'A5', 'C5', '7.9', 'B-4', '6.7', 'A4', 'G5', 'B-4', 'B-5', 'G4', '9.2', 'F#4', 'C6', 'D4', 'B-5', 'G4', '2.5.8', 'B4', '0.4.7', 'C5', '0.3.6', 'A4', '2.7', 'B-4', '9.0.2', 'F#4', '7.10.2', 'G4', '8.11', 'F4', 'G5', 'C5', 'E4', 'C#5', 'E-4', 'D5', 'D4', '3.7.10', 'C#4', '7.10.2', 'D2']\n"]}]},{"cell_type":"code","source":["!apt-get install lilypond\n"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"rA5MZDGEP_CE","executionInfo":{"status":"ok","timestamp":1745829821796,"user_tz":-330,"elapsed":6837,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"ec47aeeb-a554-4579-877b-007ef4edfa56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","^C\n"]}]},{"cell_type":"code","source":["!sudo apt-get update --fix-missing\n","\n"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"HPNwjiiaRvcy","executionInfo":{"status":"ok","timestamp":1745829821857,"user_tz":-330,"elapsed":25,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"876215a2-bad9-43aa-c0b6-fa909c73ccd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n"]}]},{"cell_type":"code","source":["!sudo nano /etc/apt/sources.list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2McMt2rSTH0","executionInfo":{"status":"ok","timestamp":1745829821943,"user_tz":-330,"elapsed":84,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"def3ee52-b89c-42f5-f7a8-4fd8c15d7f51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sudo: nano: command not found\r\n"]}]},{"cell_type":"code","source":["from music21 import note, chord, stream\n","from IPython.display import Image, display\n","\n","# Function to display the music snippet\n","def show(music):\n","    try:\n","        # Render the music snippet as an image\n","        display(Image(str(music.write(\"lily.png\"))))\n","    except:\n","        print(\"Rendering failed. Check if LilyPond is installed.\")\n","\n","# Function to convert chords and notes into a melody stream\n","def chords_n_notes(Snippet):\n","    Melody = []\n","    offset = 0  # Incremental offset\n","    for i in Snippet:\n","        # If it is a chord\n","        if (\".\" in i or i.isdigit()):\n","            chord_notes = i.split(\".\")  # Separating the notes in the chord\n","            notes = []\n","            for j in chord_notes:\n","                inst_note = int(j)\n","                note_snip = note.Note(inst_note)\n","                notes.append(note_snip)\n","            chord_snip = chord.Chord(notes)\n","            chord_snip.offset = offset\n","            Melody.append(chord_snip)\n","        # If it is a note\n","        else:\n","            note_snip = note.Note(i)\n","            note_snip.offset = offset\n","            Melody.append(note_snip)\n","        # Increase offset to avoid stacking notes\n","        offset += 1\n","    Melody_midi = stream.Stream(Melody)\n","    return Melody_midi\n","\n","# Create a melody snippet from the corpus\n","Melody_Snippet = chords_n_notes(Corpus[:100])\n","\n","# Display the melody snippet\n","show(Melody_Snippet)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iprLmJVOQgAY","executionInfo":{"status":"ok","timestamp":1745829822085,"user_tz":-330,"elapsed":140,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"6bb25d2d-4d3f-46f4-8528-50032507f39f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rendering failed. Check if LilyPond is installed.\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"2Im6ZHJ2YKa4"}},{"cell_type":"code","source":["output_dir = '/content/drive/MyDrive/music'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"],"metadata":{"id":"QzGvppSWWQOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_path = os.path.join(output_dir, 'output.mid')\n","Melody_Snippet.write('midi', fp=output_path)\n","\n","print(f\"MIDI file saved at: {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJdj3OdbYcl1","executionInfo":{"status":"ok","timestamp":1745829822169,"user_tz":-330,"elapsed":7,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"cb918be8-167e-424a-ab6a-89aa9dccccd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MIDI file saved at: /content/drive/MyDrive/music/output.mid\n"]}]},{"cell_type":"code","source":["from IPython.display import Audio\n","!apt-get install timidity\n","!pip install pydub\n","\n"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"xmP1c7qtUabV","outputId":"50059280-06ba-40c7-bd91-7a12be7ef495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"code","source":["from pydub import AudioSegment"],"metadata":{"id":"ECLzdCTkafe5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["midi_file = \"/content/drive/MyDrive/music/output.mid\"\n","wav_file = \"/content/drive/MyDrive/music/output.wav\"\n","\n","# Convert using timidity\n","os.system(f\"timidity {midi_file} -Ow -o {wav_file}\")\n","print(f\"Converted {midi_file} to {wav_file}\")"],"metadata":{"id":"X5Wks_Qzaxf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(\"Playing Converted Audio:\")\n","Audio(\"/content/drive/MyDrive/music/output.wav\")\n"],"metadata":{"id":"eP73crWMUcjl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see the total number of unique notes\n","count_num = Counter(Corpus)\n","print(\"Total unique notes in the Corpus:\", len(count_num))"],"metadata":{"id":"hv7lP9oiUgO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, there are some very rare notes in the melody; some so rare that it was played only once in the whole data. This would create a lot of problems. (I did run into most of them while writing this piece) To spare us the error reports, let us have a look at the frequency of the notes. And for simplicity, I shall be eliminating some of the least occurring notes. I am sure Chopin wouldn't mind me messing with his masterpiece for science or would he? Either way, I may never know!"],"metadata":{"id":"1njdO_Swm3N7"}},{"cell_type":"code","source":["Notes = list(count_num.keys())\n","Recurrence = list(count_num.values())\n","#Average recurrenc for a note in Corpus\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","print(\"Average recurrenc for a note in Corpus:\", Average(Recurrence))\n","print(\"Most frequent note in Corpus appeared:\", max(Recurrence), \"times\")\n","print(\"Least frequent note in Corpus appeared:\", min(Recurrence), \"time\")"],"metadata":{"id":"Qx7_MNTHd04w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(18,3),facecolor=\"#97BACB\")\n","bins = np.arange(0,(max(Recurrence)), 50)\n","plt.hist(Recurrence, bins=bins, color=\"#97BACB\")\n","plt.axvline(x=100,color=\"#DBACC1\")\n","plt.title(\"Frequency Distribution Of Notes In The Corpus\")\n","plt.xlabel(\"Frequency Of Chords in Corpus\")\n","plt.ylabel(\"Number Of Chords\")\n","plt.show()"],"metadata":{"id":"ant7CANLn9E2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#to get the number of rare notes\n","#Getting a list of rare chords\n","rare_note = []\n","for index, (key, value) in enumerate(count_num.items()):\n","    if value < 100:\n","        m =  key\n","        rare_note.append(m)\n","\n","print(\"Total number of notes that occur less than 100 times:\", len(rare_note))"],"metadata":{"id":"w9yjJbFJohWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"91JApHY7pq8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preprocessing\n","\n","> Notes are basically sound waves. In music, we have certain specific combinations of Frequency and Wavelength standardized as said notes. Our Corpus has the name of that note. As we parsed the data at the time of loading we took the help of the music21 library (by nice people at MIT); The library fetches Frequency, Wavelength, duration etc for the given notes.\n","\n","In this section, I will be performing the following:\n","\n","**Creating a dictionary:** Creating a dictionary to map the notes and their indices. We have the note's name as a string the Corpus. For the computer, these names are just a symbol. So we create a dictionary to map each unique note in our Corpus to a number. And vice versa to retrieve the values at the time of prediction. This will be used to encode and decode the information going in and getting out of the RNN.\n","\n","**Encoding and Splitting the corpus:** Encoding and splitting the corpus into smaller sequences of equal length: At this point, the Corpus contain notes. We will encode this corpus and create small sequences of equal lengths of features and the corresponding targets. Each feature and target will contain the mapped index in the dictionary of the unique characters they signify.\n","\n","**Assigning X and y**: The labels are then resized and normalized. Whereas the targets are one-hot encoded. Ready to be sent to the RNN for the training, but before that let us built the RNN model.\n","\n","Splitting Train and Seed datasets To create music we will need to send some input to the RNN. For that, we will set aside a part of the data as seeds. We could have trained it all but I am no musician to come up with an input seed value.\n","\n","\n","\n","\n"],"metadata":{"id":"-1ee2NF4p6Rg"}},{"cell_type":"code","source":["# Storing all the unique characters present in my corpus to bult a mapping dic.\n","symb = sorted(list(set(Corpus)))\n","\n","L_corpus = len(Corpus) #length of corpus\n","L_symb = len(symb) #length of total unique characters\n","\n","#Building dictionary to access the vocabulary from indices and vice versa\n","mapping = dict((c, i) for i, c in enumerate(symb))\n","reverse_mapping = dict((i, c) for i, c in enumerate(symb))\n","\n","print(\"Total number of characters:\", L_corpus)\n","print(\"Number of unique characters:\", L_symb)"],"metadata":{"id":"J5W7-em-p_jW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Mapping Dictionaries for Preprocessing:\n","The mapping dictionaries ensure that the model works with numerical data by assigning a unique integer to each musical note or chord. This conversion is essential for training neural networks, as they require input in numeric form.\n","\n","2. Understanding Corpus Diversity:\n","By analyzing the mapping, one can determine how many unique musical symbols (notes/chords) exist in the dataset. A higher number of unique symbols indicates a more diverse and richer musical corpus, which can help the model learn a wider range of musical patterns."],"metadata":{"id":"NHGPEZbKrPTa"}},{"cell_type":"code","source":["#Splitting the Corpus in equal length of strings and output target\n","length = 40\n","features = []\n","targets = []\n","for i in range(0, L_corpus - length, 1):\n","    feature = Corpus[i:i + length]\n","    target = Corpus[i + length]\n","    features.append([mapping[j] for j in feature])\n","    targets.append(mapping[target])\n","\n","\n","L_datapoints = len(targets)\n","print(\"Total number of sequences in the Corpus:\", L_datapoints)"],"metadata":{"id":"pSnlLpS9sa7b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["🎵 Sequence Preparation for Model Training\n","1. Dividing the Corpus into Smaller Sequences:\n","The corpus is a long sequence of musical notes or chords. To train the model effectively, this sequence is broken down into smaller chunks.\n","\n","Each input sequence is of fixed length (e.g., 40 notes).\n","\n","For example, if the corpus is:\n","['C4', 'D4', 'E4', 'F4', ...]\n","Then the first input feature will be:\n","['C4', 'D4', ..., 40th note]\n","And the target will be the 41st note, which the model is trained to predict.\n","\n","2. Creating Input Features and Targets:\n","Each 40-note sequence is converted into a numerical format using a mapping dictionary.\n","\n","Each unique note is assigned a number, for example:\n","'C4' → 0, 'D4' → 1, 'E4' → 2, etc.\n","\n","A feature might look like: [0, 1, 2, ..., 39]\n","and the corresponding target would be the integer representing the 41st note.\n","\n","3. Sliding Window Approach:\n","The code uses a sliding window technique to generate overlapping input-target pairs.\n","\n","At each step, the window shifts by one note, creating a new 40-note input and the following note as the target.\n","\n","This approach increases the number of training samples and helps the model learn better patterns from overlapping contexts.\n","\n","4. Calculating Total Sequences:\n","Finally, the code calculates how many total input-target pairs have been created from the corpus.\n","\n","This helps in understanding the size of the training dataset and gives insight into how much data the model has seen."],"metadata":{"id":"6zv_rbbNsYTS"}},{"cell_type":"code","source":["# reshape X and normalize\n","X = (np.reshape(features, (L_datapoints, length, 1)))/ float(L_symb)\n","# one hot encode the output variable\n","y = tensorflow.keras.utils.to_categorical(targets)"],"metadata":{"id":"CqOpHHyarcmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Input Data Preparation for LSTM Model\n","1. Understanding the Input (Features):\n","The features variable is a 2D list where each row represents a sequence of musical notes (input features).\n","\n","Shape before processing: (num_sequences, sequence_length)\n","\n","num_sequences: Total number of input-target pairs generated from the corpus.\n","\n","sequence_length: The fixed length of each input sequence (e.g., 40 notes).\n","\n","2. Adding the Third Dimension (Reshaping):\n","To make the input compatible with LSTM layers, the data is reshaped into a 3D\n","#3.Normalization:\n","Normalization scales all input values to a small range, typically between 0 and 1, to improve training efficiency.\n","\n","Done by dividing each input value by the total number of unique notes/symbols (num_unique_notes or L_symb).\n","##4. Why 3D Input is Required:\n","LSTM and other recurrent models expect inputs in 3D format:\n","\n","Batch Size = Number of sequences (num_sequences)\n","\n","Time Steps = Length of each sequence (sequence_length)\n","\n","Features per Time Step = Number of features at each step (1 here, since each note is a scalar)\n","\n","This 3D structure enables the model to process temporal (sequential) dependencies effectively."],"metadata":{"id":"Xf1wSfEaug-m"}},{"cell_type":"code","source":["X_train, X_seed, y_train, y_seed = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"_9tqth0cu_PQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# model building\n","\n","### we will be employing an LSTM for this project.\n","\n","Following steps are involved in the model building\n","\n","Initialising the Model\n","Defining by adding layers\n","Compiling the Model\n","Training the Model"],"metadata":{"id":"J4Hcu7cOvEO9"}},{"cell_type":"code","source":["#Initialising the Model\n","model = Sequential()\n","#Adding layers\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.1))\n","model.add(LSTM(128))\n","model.add(Dense(128))\n","model.add(Dropout(0.1))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","#Compiling the model for training\n","opt = Adamax(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=opt)"],"metadata":{"id":"Werg64fcujHp","executionInfo":{"status":"error","timestamp":1745830304928,"user_tz":-330,"elapsed":112,"user":{"displayName":"Taru Tiwari","userId":"01557451727370636724"}},"outputId":"bc040c70-d1ed-4a7e-a601-96acb3bc05a7","colab":{"base_uri":"https://localhost:8080/","height":218}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Sequential' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a48fbbb9a8f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Initialising the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Adding layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"]}]},{"cell_type":"code","source":["# prompt: model.Summary()\n","\n","model.summary()\n"],"metadata":{"id":"6wJSV-FTwK6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training the Model\n","history = model.fit(X_train, y_train, batch_size=128, epochs=20)"],"metadata":{"id":"Y4ed3xPXt3Gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plotting the learnings\n","history_df = pd.DataFrame(history.history)\n","fig = plt.figure(figsize=(15,4), facecolor=\"#97BACB\")\n","fig.suptitle(\"Learning Plot of Model for Loss\")\n","pl=sns.lineplot(data=history_df[\"loss\"],color=\"#444160\")\n","pl.set(ylabel =\"Training Loss\")\n","pl.set(xlabel =\"Epochs\")"],"metadata":{"id":"wE0WATFMwiVP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["this sure looks like music! To check if it sounds like music we have to listen to the MIDI file. Playing midi is crumblesome. I have saved and converted a few generated melodies to \".wav\" format outside of this notebook. So let us have a listen."],"metadata":{"id":"kVnr6OPhvraa"}},{"cell_type":"code","source":["def Melody_Generator(Note_Count):\n","    seed = X_seed[np.random.randint(0,len(X_seed)-1)]\n","    Music = \"\"\n","    Notes_Generated=[]\n","    for i in range(Note_Count):\n","        seed = seed.reshape(1,length,1)\n","        prediction = model.predict(seed, verbose=0)[0]\n","        prediction = np.log(prediction) / 1.0 #diversity\n","        exp_preds = np.exp(prediction)\n","        prediction = exp_preds / np.sum(exp_preds)\n","        index = np.argmax(prediction)\n","        index_N = index/ float(L_symb)\n","        Notes_Generated.append(index)\n","        Music = [reverse_mapping[char] for char in Notes_Generated]\n","        seed = np.insert(seed[0],len(seed[0]),index_N)\n","        seed = seed[1:]\n","        #Now, we have music in form or a list of chords and notes and we want to be a midi file.\n","    Melody = chords_n_notes(Music)\n","    Melody_midi = stream.Stream(Melody)\n","    return Music,Melody_midi\n","\n","\n","#getting the Notes and Melody created by the model\n","Music_notes, Melody = Melody_Generator(100)\n","show(Melody)\n"],"metadata":{"id":"I_ZoLBc8r9If"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#To save the generated melody\n","Melody.write('midi','/content/drive/MyDrive/music/Melody_Generated.mid')\n"],"metadata":{"id":"5S2GzvXEu-dN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!timidity /content/drive/MyDrive/music/Melody_Generated.mid -Ow -o /content/drive/MyDrive/music/Melody_Generated.wav\n"],"metadata":{"id":"Zj2QY2s1xX_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Audio\n","Audio('/content/drive/MyDrive/music/Melody_Generated.wav')\n"],"metadata":{"id":"g6g1LATgyOqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/drive/MyDrive/music/Melody_Generated.mid')\n","\n","\n"],"metadata":{"id":"gsmJEYeHyS8j"},"execution_count":null,"outputs":[]}]}